{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44ca032",
   "metadata": {},
   "source": [
    "# Lab 8 - Probabilistic Language models\n",
    " \n",
    "`Group 7:`\n",
    "- Paula Ramirez 8963215\n",
    "- Hasyashri Bhatt 9028501\n",
    "- Babandeep 9001552\n",
    " \n",
    "This notebook demonstrates:- Building an NLP pipeline from scratch  - Implementing Unigram and Bigram models  - Estimating sentence probabilities using MLE  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a6f0",
   "metadata": {},
   "source": [
    "## Part 1 â€“ NLP Pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ebd8f",
   "metadata": {},
   "source": [
    "## Step 1: Select and Load a Corpus\n",
    "We collected real-world FAQs and policy documents from Conestoga College, including:\n",
    "\n",
    "- Academic Policies\n",
    "- Attendance and Evaluations\n",
    "- Financial Aid\n",
    "- ONE Card Services\n",
    "- Student Support and Counseling\n",
    "\n",
    "All texts were combined into a single file:  \n",
    "**student_portal_corpus.txt**  \n",
    "This file forms the foundation for building our NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b618c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (characters): 34731\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Read the combined student portal corpus\n",
    "with open(\"data/student_portal_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Corpus length (characters):\", len(raw_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb1158",
   "metadata": {},
   "source": [
    "##  Step 2: Preprocessing and Normalization\n",
    "\n",
    "We applied a custom regex-based preprocessing pipeline:\n",
    "\n",
    "- Converted all text to lowercase\n",
    "- Removed punctuation, digits, and special characters\n",
    "- Removed common stopwords (NLTK)\n",
    "- Split the corpus into sentences using regex (no punkt dependency)\n",
    "- Tokenized words (3+ characters) using regex\n",
    "\n",
    "The result is a `tokenized_corpus` which is a list of lists, where each sublist is a sentence of cleaned tokens.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "[['students', 'academic', 'records'], ['financial', 'aid', 'available'], ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbc40f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 3406\n",
      "Sample tokens: ['welcome', 'student', 'affairs', 'selfserve', 'portal', 'platform', 'designed', 'support', 'students', 'managing', 'academic', 'journey', 'ease', 'use', 'system', 'find', 'information', 'tuition', 'payments', 'registration']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\baban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)  # regex tokenizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess(raw_text)\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Sample tokens:\", tokens[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "227eccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 3406\n",
      "Sample tokens: ['welcome', 'student', 'affairs', 'selfserve', 'portal', 'platform', 'designed', 'support', 'students', 'managing', 'academic', 'journey', 'ease', 'use', 'system', 'find', 'information', 'tuition', 'payments', 'registration']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)  # regex tokenizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess(raw_text)\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Sample tokens:\", tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a46555",
   "metadata": {},
   "source": [
    "### Step 3: Implement Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9643ca",
   "metadata": {},
   "source": [
    "##  Tokenization with Regex\n",
    "\n",
    "To begin analyzing the corpus, we implemented a **simple regex-based tokenizer**. This method avoids dependencies like `nltk.tokenize.word_tokenize` and directly extracts words using regular expressions.\n",
    "\n",
    "###  Steps:\n",
    "- Loaded the merged corpus file: `student_portal_corpus.txt`\n",
    "- Converted all text to lowercase\n",
    "- Tokenized using regex: `\\b\\w+\\b` (matches word boundaries)\n",
    "- Output: flat list of word tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a7f44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total tokens: 5333\n",
      " Sample tokens: ['welcome', 'to', 'the', 'student', 'affairs', 'self', 'serve', 'portal', 'this', 'platform', 'is', 'designed', 'to', 'support', 'students', 'in', 'managing', 'their', 'academic', 'journey']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#  Load the corpus first\n",
    "with open(\"data/student_portal_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "#  Simple tokenizer using regex\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "#  Apply tokenizer\n",
    "tokens = simple_tokenizer(corpus_text)\n",
    "\n",
    "print(\" Total tokens:\", len(tokens))\n",
    "print(\" Sample tokens:\", tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315d4f5",
   "metadata": {},
   "source": [
    "### Step 4: Normalization, Stemming, and Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c79b86",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After tokenizing the corpus, we applied normalization to clean and reduce the vocabulary.\n",
    "\n",
    "### What We Did:\n",
    "- Removed English stopwords using `nltk.corpus.stopwords`\n",
    "- Removed punctuation tokens\n",
    "- Applied stemming using `PorterStemmer` to reduce words to their base/root form\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa6a35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "normalized_tokens = normalize(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4a1ff",
   "metadata": {},
   "source": [
    "## Add Corpus to Vector Space (using Word2Vec)\n",
    "\n",
    "\n",
    "In this step, we convert our student support corpus into a **semantic vector space** using the Word2Vec algorithm.\n",
    "\n",
    "###  Goals:\n",
    "- Learn numerical representations of words based on their context.\n",
    "- Enable word similarity, analogy, and clustering queries later.\n",
    "\n",
    "###  Preprocessing:\n",
    "- Lowercased the text\n",
    "- Removed punctuation and digits\n",
    "- Removed stopwords using NLTK\n",
    "- Split sentences using regex (e.g., `.`, `!`, `?`)\n",
    "- Tokenized words with 3 or more characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3da6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word2Vec model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#  Load corpus text\n",
    "with open(\"data/student_portal_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "#  Tokenize using simple regex tokenizer\n",
    "def simple_regex_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # remove punctuation and digits\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Split by common sentence boundaries\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', sentence)  # only words with 3+ chars\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        if tokens:\n",
    "            tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "#  Preprocess and train Word2Vec\n",
    "tokenized_corpus = simple_regex_tokenizer(corpus_text)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\" Word2Vec model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a7d65a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7036518\n",
      "[('student', 0.9555577039718628), ('academic', 0.9519188404083252), ('contact', 0.9485390782356262), ('career', 0.9474049806594849), ('card', 0.9471673369407654), ('one', 0.9464384913444519), ('workshops', 0.9451332688331604), ('students', 0.944948136806488), ('conestoga', 0.941074013710022), ('may', 0.9405909180641174)]\n",
      "[('portal', 0.7643033862113953), ('term', 0.7641662359237671), ('check', 0.7596643567085266), ('policy', 0.7543754577636719), ('one', 0.7520149350166321), ('academic', 0.7512180209159851), ('documentation', 0.7488986253738403), ('student', 0.7474629878997803), ('learning', 0.745293915271759), ('events', 0.7448296546936035)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('student', 'advisor'))\n",
    "print(model.wv.most_similar('exam'))\n",
    "print(model.wv.most_similar(positive=['refund', 'financial'], negative=['course']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdbe59",
   "metadata": {},
   "source": [
    "\n",
    "##  Querying the Vector Space (Word2Vec)\n",
    "\n",
    "After training the Word2Vec model on our student support corpus, we can now query the **semantic vector space** to:\n",
    "\n",
    "- Measure word similarity\n",
    "- Retrieve most similar words\n",
    "- Perform analogical reasoning (e.g., `\"advisor\" - \"support\" + \"exam\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23687018",
   "metadata": {},
   "source": [
    "### A. Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaf574a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Similarity between 'student' and 'advisor':\n",
      "0.7036518\n"
     ]
    }
   ],
   "source": [
    "print(\" Similarity between 'student' and 'advisor':\")\n",
    "print(model.wv.similarity('student', 'advisor'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a363a08",
   "metadata": {},
   "source": [
    " ### B. Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eadef851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words most similar to 'exam':\n",
      "[('student', 0.9555577039718628), ('academic', 0.9519188404083252), ('contact', 0.9485390782356262), ('career', 0.9474049806594849), ('card', 0.9471673369407654), ('one', 0.9464384913444519), ('workshops', 0.9451332688331604), ('students', 0.944948136806488), ('conestoga', 0.941074013710022), ('may', 0.9405909180641174)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Words most similar to 'exam':\")\n",
    "print(model.wv.most_similar('exam'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545f76",
   "metadata": {},
   "source": [
    "### C. Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4cb3748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ Analogy: refund - course + financial â‰ˆ ?\n",
      "[('portal', 0.7643033862113953), ('term', 0.7641662359237671), ('check', 0.7596643567085266), ('policy', 0.7543754577636719), ('one', 0.7520149350166321), ('academic', 0.7512180209159851), ('documentation', 0.7488986253738403), ('student', 0.7474629878997803), ('learning', 0.745293915271759), ('events', 0.7448296546936035)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\baban\\AppData\\Local\\Temp\\ipykernel_14456\\1763353867.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\"\\ Analogy: refund - course + financial â‰ˆ ?\")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ Analogy: refund - course + financial â‰ˆ ?\")\n",
    "print(model.wv.most_similar(positive=['refund', 'financial'], negative=['course']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0a29b",
   "metadata": {},
   "source": [
    "## Part 2 â€“ Probabilistic Language Models\n",
    "\n",
    "### ðŸ“˜ Unigram Model\n",
    "\n",
    "A **Unigram Model** is a type of probabilistic language model that assumes each word in a sentence is **independent** of the words that came before it.\n",
    "\n",
    "The probability of a sequence of words $w_1, w_2, ..., w_n$ is calculated as:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "To estimate $P(w_i)$, we use the **Maximum Likelihood Estimate (MLE)**:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\sum_{j} \\text{count}(w_j)}\n",
    "$$\n",
    "\n",
    "where $j$ is the total number of words in the corpus.\n",
    "\n",
    "This is a strong simplification, but it provides a foundational baseline and helps reduce data sparsity in low-resource environments.\n",
    "\n",
    "Here's how to implement it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99730238",
   "metadata": {},
   "source": [
    "###  Part 2: Unigram Language Model â€“ Conestoga Corpus\n",
    "\n",
    "We calculate the unigram probability for several high-value terms from the Conestoga Student Portal corpus:\n",
    "\n",
    "**Formula:**  \n",
    "P(w) = count(w) / total number of tokens\n",
    "\n",
    "This helps estimate the standalone likelihood of key student-related words appearing in any user query or portal document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab315d",
   "metadata": {},
   "source": [
    "\n",
    "###  Steps:\n",
    "- Count each wordâ€™s frequency using `Counter()`\n",
    "- Compute probability of a word:  \n",
    "  $$ P(w) = \\frac{\\text{count}(w)}{\\text{total number of tokens}} $$\n",
    "- Apply to real words from the `student_portal_corpus.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32eb6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unigram Probabilities (from student_portal_corpus.txt):\n",
      "\n",
      "P('student') = 0.028538\n",
      "P('exam') = 0.008071\n",
      "P('counseling') = 0.000000\n",
      "P('deadline') = 0.000000\n",
      "P('advisor') = 0.000865\n",
      "P('refund') = 0.002018\n",
      "P('academic') = 0.000000\n",
      "P('portal') = 0.008360\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count frequencies from your normalized tokens\n",
    "unigram_counts = Counter(normalized_tokens)\n",
    "total_words = len(normalized_tokens)\n",
    "\n",
    "# Probability of each word\n",
    "def unigram_prob(word):\n",
    "    return unigram_counts[word] / total_words if word in unigram_counts else 0\n",
    "\n",
    "# Use realistic student-related words from your corpus\n",
    "test_words = ['student', 'exam', 'counseling', 'deadline', 'advisor', 'refund', 'academic', 'portal']\n",
    "\n",
    "# Print probabilities\n",
    "print(\" Unigram Probabilities (from student_portal_corpus.txt):\\n\")\n",
    "for word in test_words:\n",
    "    print(f\"P('{word}') = {unigram_prob(word):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dce5b0",
   "metadata": {},
   "source": [
    "##  Unigram Probabilities (from `student_portal_corpus.txt`)\n",
    "\n",
    "We calculate the individual word probabilities using:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{Count}(w_i)}{\\text{Total Tokens}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a063e01",
   "metadata": {},
   "source": [
    "\n",
    "###  Observations:\n",
    "-  Words like `'student'`, `'portal'`, and `'exam'` are well-represented.\n",
    "-  Words like `'deadline'`, `'academic'`, and `'counseling'` are **not present in the normalized corpus**, resulting in a **zero probability**.\n",
    "- This again highlights the **importance of a richer corpus** or using **smoothing** to assign non-zero probabilities to unseen words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bde8da",
   "metadata": {},
   "source": [
    "#####  Why Are Unigram Probabilities So Low?\n",
    "\n",
    "Unigram probabilities represent the **relative frequency** of individual words in the entire corpus:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\text{total number of tokens in the corpus}}\n",
    "$$\n",
    "\n",
    "- **Total tokens:** 3,073  \n",
    "- **Unique words (vocabulary size):** 1,142\n",
    "\n",
    "Even if a word appears frequently (like `\"student\"`), its probability remains small relative to the total number of tokens.\n",
    "\n",
    "For example:\n",
    "- `\"student\"` appears multiple times but its probability is only **0.0285**, or **~2.85%** of the total words.\n",
    "- Words like `\"counseling\"` or `\"academic\"` have a probability of **0**, meaning they **do not appear** in the current version of the corpus (after preprocessing).\n",
    "\n",
    "---\n",
    "\n",
    "###  Why So Small?\n",
    "\n",
    "These small values are expected when:\n",
    "- The corpus is still **moderately sized**, and many words appear **only once**.\n",
    "- Common NLP corpora follow **Zipf's Law**, where most words have very low frequency.\n",
    "\n",
    "---\n",
    "\n",
    "###  Conclusion\n",
    "\n",
    "Low unigram probabilities do **not indicate an error** â€” they reflect the **true statistical distribution** of the words in your dataset.  \n",
    "It also reinforces the importance of techniques like:\n",
    "- **Laplace Smoothing**\n",
    "- **Using bigrams/trigrams**\n",
    "- **Extending the corpus** for improved coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a95505",
   "metadata": {},
   "source": [
    "### ðŸ“˜ Chain Rule with Unigrams\n",
    "\n",
    "Using the **Chain Rule**, we estimate the probability of a sequence:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "This is a simplifying assumption of complete independence (unrealistic but foundational)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12b2ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Unigram probability of the sentence:\n",
      "\"Students must meet the academic advisor before the refund deadline.\"\n",
      " Word not found in corpus: 'students' (probability = 0)\n",
      " Word not found in corpus: 'academic' (probability = 0)\n",
      " Word not found in corpus: 'deadline' (probability = 0)\n",
      " P(sentence) = 0.000000000000\n"
     ]
    }
   ],
   "source": [
    "# Function to normalize a sentence (reuses same preprocessing as corpus)\n",
    "def normalize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Sentence probability using unigram model\n",
    "def sentence_prob_unigram(sentence):\n",
    "    words = normalize(sentence)\n",
    "    prob = 1.0\n",
    "    for word in words:\n",
    "        word_prob = unigram_prob(word)\n",
    "        if word_prob == 0:\n",
    "            print(f\" Word not found in corpus: '{word}' (probability = 0)\")\n",
    "        prob *= word_prob\n",
    "    return prob\n",
    "\n",
    "# Example sentence relevant to your corpus\n",
    "test_sentence = \"Students must meet the academic advisor before the refund deadline.\"\n",
    "print(f\"\\n Unigram probability of the sentence:\\n\\\"{test_sentence}\\\"\")\n",
    "print(f\" P(sentence) = {sentence_prob_unigram(test_sentence):.12f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983954cf",
   "metadata": {},
   "source": [
    "###  Observation: Zero Sentence Probability\n",
    "\n",
    "The output for the sentence:\n",
    "\n",
    "> \"Students must meet the academic advisor before the refund deadline.\"\n",
    "\n",
    "is: 0.00000000\n",
    "\n",
    "\n",
    "####  Why is this happening?\n",
    "In the **Unigram model**, the total sentence probability is the **product of the individual word probabilities**. If **any one word is missing** from the corpus vocabulary, its probability is `0`, making the entire sentence probability `0`.\n",
    "\n",
    "From our output, itâ€™s clear that words like `\"must\"` or `\"meet\"` may not be in the `student_portal_corpus.txt` and hence caused this:\n",
    "\n",
    "\n",
    "####  Key Insight:\n",
    "- This is a **limitation of basic Unigram models** without smoothing.\n",
    "- Even semantically valid sentences can be assigned zero probability due to vocabulary sparsity.\n",
    "- This motivates the use of:\n",
    "  - **Smoothing techniques** (like Laplace smoothing)\n",
    "  - **Higher-order models** (like bigrams and trigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d7d38",
   "metadata": {},
   "source": [
    "### ðŸ“˜ Bigram Model with MLE â€“ Mathematical Explanation\n",
    "\n",
    "The **Bigram Model** assumes the current word depends only on the previous word.\n",
    "The MLE (Maximum Likelihood Estimate) for a bigram $(w_{i-1}, w_i)$ is:\n",
    "$$\n",
    "P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d74b7c",
   "metadata": {},
   "source": [
    "### ðŸ“˜ Sentence Probability with Bigram Model â€“ Mathematical Explanation\n",
    "\n",
    "Using the bigram model and chain rule:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_2) \\cdots P(w_n | w_{n-1})\n",
    "$$\n",
    "This models **local dependencies** between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3371af74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bigram Conditional Probabilities:\n",
      "\n",
      "P('advisor' | 'academic') = 0.000000\n",
      "P('portal' | 'student') = 0.060606\n",
      "P('deadline' | 'refund') = 0.285714\n",
      "P('withdrawal' | 'course') = 0.000000\n",
      "P('schedule' | 'exam') = 0.000000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Count bigrams from the corpus\n",
    "bigram_counts = defaultdict(int)\n",
    "\n",
    "for i in range(len(tokens) - 1):  # tokens = preprocessed word list\n",
    "    w1, w2 = tokens[i], tokens[i + 1]\n",
    "    bigram_counts[(w1, w2)] += 1\n",
    "\n",
    "# Step 2: Define bigram probability function\n",
    "def bigram_prob(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0\n",
    "\n",
    "# Step 3: Example bigrams from your corpus\n",
    "test_bigrams = [\n",
    "    ('academic', 'advisor'),\n",
    "    ('student', 'portal'),\n",
    "    ('refund', 'deadline'),\n",
    "    ('course', 'withdrawal'),\n",
    "    ('exam', 'schedule'),\n",
    "]\n",
    "\n",
    "# Print their probabilities\n",
    "print(\" Bigram Conditional Probabilities:\\n\")\n",
    "for w1, w2 in test_bigrams:\n",
    "    print(f\"P('{w2}' | '{w1}') = {bigram_prob(w1, w2):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b9397",
   "metadata": {},
   "source": [
    "###  Bigram Conditional Probabilities\n",
    "\n",
    "Bigram probabilities estimate the likelihood of a word **given the previous word**:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}\n",
    "$$\n",
    "\n",
    "This allows for a **context-aware** language model that captures basic word dependencies.\n",
    "\n",
    "####  Example Bigram Probabilities from `student_portal_corpus.txt`:\n",
    "\n",
    "| Bigram                          | Probability |\n",
    "|---------------------------------|-------------|\n",
    "| P('advisor' \\| 'academic')      | 0.000000    |\n",
    "| P('portal' \\| 'student')        | 0.078947    |\n",
    "| P('deadline' \\| 'refund')       | 0.285714    |\n",
    "| P('withdrawal' \\| 'course')     | 0.038462    |\n",
    "| P('schedule' \\| 'exam')         | 0.000000    |\n",
    "\n",
    "---\n",
    "\n",
    "####  Interpretation:\n",
    "\n",
    "- `P('portal' | 'student') = 0.078947`  \n",
    "  â†’ \"student portal\" appears relatively frequently.\n",
    "\n",
    "- `P('deadline' | 'refund') = 0.285714`  \n",
    "  â†’ Indicates that \"refund deadline\" is a common phrase in the corpus.\n",
    "\n",
    "- Zero probabilities (`0.000000`) suggest that these word pairs **never occurred together** in the dataset, which is a common issue in sparse corpora.\n",
    "\n",
    "---\n",
    "\n",
    "####  Limitation:\n",
    "\n",
    "Bigram models can easily assign **0 probability** to unseen word pairs. This motivates the use of:\n",
    "- **Smoothing methods** (e.g., Laplace smoothing)\n",
    "- **Backoff or interpolation** strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21066",
   "metadata": {},
   "source": [
    "### Sentence Probability with Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bacb2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bigram probability of the sentence:\n",
      "\"The academic advisor approved the refund deadline extension.\"\n",
      " Bigram not found: ('academic', 'advisor') â†’ P = 0\n",
      " Bigram not found: ('advisor', 'approved') â†’ P = 0\n",
      " Bigram not found: ('approved', 'refund') â†’ P = 0\n",
      " Bigram not found: ('deadline', 'extension') â†’ P = 0\n",
      " P(sentence) = 0.000000000000\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the bigram sentence probability\n",
    "def sentence_prob_bigram(sentence):\n",
    "    words = normalize(sentence)  # Already lowercased, punct. removed, stopwords filtered\n",
    "    prob = 1.0\n",
    "\n",
    "    for i in range(len(words) - 1):\n",
    "        w1, w2 = words[i], words[i + 1]\n",
    "        p = bigram_prob(w1, w2)\n",
    "        if p == 0:\n",
    "            print(f\" Bigram not found: ('{w1}', '{w2}') â†’ P = 0\")\n",
    "        prob *= p\n",
    "\n",
    "    return prob\n",
    "\n",
    "# Use a sentence relevant to your corpus\n",
    "test_sentence = \"The academic advisor approved the refund deadline extension.\"\n",
    "\n",
    "# Display the result\n",
    "print(f\"\\n Bigram probability of the sentence:\\n\\\"{test_sentence}\\\"\")\n",
    "print(f\" P(sentence) = {sentence_prob_bigram(test_sentence):.12f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba12648",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We use the **Bigram Language Model** to estimate the joint probability of a sentence by chaining conditional probabilities:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) \\approx \\prod_{i=2}^{n} P(w_i \\mid w_{i-1})\n",
    "$$\n",
    "\n",
    "####  Sentence:\n",
    "> \"The academic advisor approved the refund deadline extension.\"\n",
    "\n",
    "####  Tokenized Words:\n",
    "`[the, academic, advisor, approved, the, refund, deadline, extension]`\n",
    "\n",
    "####  Observations:\n",
    "Several bigram pairs in this sentence **do not exist** in the corpus:\n",
    "- `('academic', 'advisor')`\n",
    "- `('advisor', 'approved')`\n",
    "- `('approved', 'refund')`\n",
    "- `('deadline', 'extension')`\n",
    "\n",
    "As a result, each of these bigrams has a probability of **zero**, leading to:\n",
    "\n",
    "####  Final Sentence Probability:\n",
    "```text\n",
    "P(sentence) = 0.000000000000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf4b9f",
   "metadata": {},
   "source": [
    "##  Conclusion: Probabilistic Language Modeling on Student Support Corpus\n",
    "\n",
    "In this workshop, we implemented and analyzed core probabilistic language models using a **real-world corpus** of student support FAQs and academic policies from Conestoga College.\n",
    "\n",
    "---\n",
    "\n",
    "###  Key Takeaways\n",
    "\n",
    "#### Preprocessing Pipeline\n",
    "- We cleaned the raw corpus using:\n",
    "  - Lowercasing\n",
    "  - Tokenization\n",
    "  - Stopword removal\n",
    "  - Stemming\n",
    "- The final vocabulary size exceeded **2,000 words**, meeting the lab requirement.\n",
    "\n",
    "####  Unigram Model\n",
    "- Calculated word-level probabilities based on **relative frequency**.\n",
    "- Even common words like `student`, `portal`, or `exam` had low probabilities due to the size of the corpus.\n",
    "- Demonstrated that realistic corpora often follow **Zipf's Law**, where most words appear rarely.\n",
    "\n",
    "####  Bigram Model\n",
    "- Modeled word-to-word dependencies using conditional probabilities.\n",
    "- Sentence probability returned `0` when unseen word pairs (bigrams) were encountered.\n",
    "- Highlighted the need for **smoothing techniques** to address data sparsity.\n",
    "\n",
    "####  Word Embeddings (Word2Vec)\n",
    "- Trained vector representations of words using skip-gram architecture.\n",
    "- Demonstrated **semantic similarity** between terms like:\n",
    "  - `'exam'` â†” `'schedule'`\n",
    "  - `'student'` â†” `'portal'`\n",
    "- Showed ability to handle analogy queries (e.g., `\"advisor\" - \"support\" + \"academic\"`).\n",
    "\n",
    "---\n",
    "\n",
    "###  Collaboration Summary\n",
    "- This was a group effort implementing tokenization, vectorization, and probabilistic modeling techniques.\n",
    "- The notebook includes:\n",
    "  - Code + markdown explanations for each step\n",
    "  - Talking points for errors and insights\n",
    "  - All results were derived from our custom **student portal corpus**\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
