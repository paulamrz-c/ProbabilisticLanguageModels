{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44ca032",
   "metadata": {},
   "source": [
    "# Lab 8 - Probabilistic Language models\n",
    " \n",
    "`Group 7:`\n",
    "- Paula Ramirez 8963215\n",
    "- Hasyashri Bhatt 9028501\n",
    "- Babandeep 9001552\n",
    " \n",
    "This notebook demonstrates:- Building an NLP pipeline from scratch  - Implementing Unigram and Bigram models  - Estimating sentence probabilities using MLE  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a6f0",
   "metadata": {},
   "source": [
    "## Part 1 ‚Äì NLP Pipeline\n",
    "\n",
    "### Step 1: Select and Load a Corpus\n",
    "\n",
    "Select a corpus from `nltk`, or upload your own text documents. Ensure your vocabulary size exceeds 2000 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ebd8f",
   "metadata": {},
   "source": [
    "## Step 1: Document Collection\n",
    "\n",
    "We collected real-world FAQs and policy documents from Conestoga College, including:\n",
    "\n",
    "- Academic Policies\n",
    "- Attendance and Evaluations\n",
    "- Financial Aid\n",
    "- ONE Card Services\n",
    "- Student Support and Counseling\n",
    "\n",
    "All texts were combined into a single file:  \n",
    "**student_portal_corpus.txt**  \n",
    "This file forms the foundation for building our NLP models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b618c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length (characters): 34731\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Read the combined student portal corpus\n",
    "with open(\"data/student_portal_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Corpus length (characters):\", len(raw_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb1158",
   "metadata": {},
   "source": [
    "##  Step 2: Preprocessing and Normalization\n",
    "\n",
    "We applied a custom regex-based preprocessing pipeline:\n",
    "\n",
    "- Converted all text to lowercase\n",
    "- Removed punctuation, digits, and special characters\n",
    "- Removed common stopwords (NLTK)\n",
    "- Split the corpus into sentences using regex (no punkt dependency)\n",
    "- Tokenized words (3+ characters) using regex\n",
    "\n",
    "The result is a `tokenized_corpus` which is a list of lists, where each sublist is a sentence of cleaned tokens.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "[['students', 'academic', 'records'], ['financial', 'aid', 'available'], ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbc40f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 3406\n",
      "Sample tokens: ['welcome', 'student', 'affairs', 'selfserve', 'portal', 'platform', 'designed', 'support', 'students', 'managing', 'academic', 'journey', 'ease', 'use', 'system', 'find', 'information', 'tuition', 'payments', 'registration']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\baban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)  # regex tokenizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess(raw_text)\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Sample tokens:\", tokens[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "227eccc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 3406\n",
      "Sample tokens: ['welcome', 'student', 'affairs', 'selfserve', 'portal', 'platform', 'designed', 'support', 'students', 'managing', 'academic', 'journey', 'ease', 'use', 'system', 'find', 'information', 'tuition', 'payments', 'registration']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # remove punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)  # regex tokenizer\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess(raw_text)\n",
    "print(\"Total tokens:\", len(tokens))\n",
    "print(\"Sample tokens:\", tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a46555",
   "metadata": {},
   "source": [
    "### Step 3: Implement Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9643ca",
   "metadata": {},
   "source": [
    "##  Tokenization with Regex\n",
    "\n",
    "To begin analyzing the corpus, we implemented a **simple regex-based tokenizer**. This method avoids dependencies like `nltk.tokenize.word_tokenize` and directly extracts words using regular expressions.\n",
    "\n",
    "###  Steps:\n",
    "- Loaded the merged corpus file: `student_portal_corpus.txt`\n",
    "- Converted all text to lowercase\n",
    "- Tokenized using regex: `\\b\\w+\\b` (matches word boundaries)\n",
    "- Output: flat list of word tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a7f44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total tokens: 5333\n",
      " Sample tokens: ['welcome', 'to', 'the', 'student', 'affairs', 'self', 'serve', 'portal', 'this', 'platform', 'is', 'designed', 'to', 'support', 'students', 'in', 'managing', 'their', 'academic', 'journey']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#  Load the corpus first\n",
    "with open(\"data/student_portal_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "#  Simple tokenizer using regex\n",
    "def simple_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "#  Apply tokenizer\n",
    "tokens = simple_tokenizer(corpus_text)\n",
    "\n",
    "print(\" Total tokens:\", len(tokens))\n",
    "print(\" Sample tokens:\", tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315d4f5",
   "metadata": {},
   "source": [
    "### Step 4: Normalization, Stemming, and Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c79b86",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After tokenizing the corpus, we applied normalization to clean and reduce the vocabulary.\n",
    "\n",
    "### What We Did:\n",
    "- Removed English stopwords using `nltk.corpus.stopwords`\n",
    "- Removed punctuation tokens\n",
    "- Applied stemming using `PorterStemmer` to reduce words to their base/root form\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa6a35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "normalized_tokens = normalize(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4a1ff",
   "metadata": {},
   "source": [
    "## Add Corpus to Vector Space (using Word2Vec)\n",
    "\n",
    "\n",
    "In this step, we convert our student support corpus into a **semantic vector space** using the Word2Vec algorithm.\n",
    "\n",
    "###  Goals:\n",
    "- Learn numerical representations of words based on their context.\n",
    "- Enable word similarity, analogy, and clustering queries later.\n",
    "\n",
    "###  Preprocessing:\n",
    "- Lowercased the text\n",
    "- Removed punctuation and digits\n",
    "- Removed stopwords using NLTK\n",
    "- Split sentences using regex (e.g., `.`, `!`, `?`)\n",
    "- Tokenized words with 3 or more characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3da6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word2Vec model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#  Load corpus text\n",
    "with open(\"data/student_portal_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "#  Tokenize using simple regex tokenizer\n",
    "def simple_regex_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)  # remove punctuation and digits\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Split by common sentence boundaries\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(r'\\b[a-zA-Z]{3,}\\b', sentence)  # only words with 3+ chars\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        if tokens:\n",
    "            tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "#  Preprocess and train Word2Vec\n",
    "tokenized_corpus = simple_regex_tokenizer(corpus_text)\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\" Word2Vec model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a7d65a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7036518\n",
      "[('student', 0.9555577039718628), ('academic', 0.9519188404083252), ('contact', 0.9485390782356262), ('career', 0.9474049806594849), ('card', 0.9471673369407654), ('one', 0.9464384913444519), ('workshops', 0.9451332688331604), ('students', 0.944948136806488), ('conestoga', 0.941074013710022), ('may', 0.9405909180641174)]\n",
      "[('portal', 0.7643033862113953), ('term', 0.7641662359237671), ('check', 0.7596643567085266), ('policy', 0.7543754577636719), ('one', 0.7520149350166321), ('academic', 0.7512180209159851), ('documentation', 0.7488986253738403), ('student', 0.7474629878997803), ('learning', 0.745293915271759), ('events', 0.7448296546936035)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('student', 'advisor'))\n",
    "print(model.wv.most_similar('exam'))\n",
    "print(model.wv.most_similar(positive=['refund', 'financial'], negative=['course']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdbe59",
   "metadata": {},
   "source": [
    "\n",
    "##  Querying the Vector Space (Word2Vec)\n",
    "\n",
    "After training the Word2Vec model on our student support corpus, we can now query the **semantic vector space** to:\n",
    "\n",
    "- Measure word similarity\n",
    "- Retrieve most similar words\n",
    "- Perform analogical reasoning (e.g., `\"advisor\" - \"support\" + \"exam\"`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23687018",
   "metadata": {},
   "source": [
    "### 4A. Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eaf574a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Similarity between 'student' and 'advisor':\n",
      "0.7036518\n"
     ]
    }
   ],
   "source": [
    "print(\" Similarity between 'student' and 'advisor':\")\n",
    "print(model.wv.similarity('student', 'advisor'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a363a08",
   "metadata": {},
   "source": [
    " ### 4B. Most Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eadef851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words most similar to 'exam':\n",
      "[('student', 0.9555577039718628), ('academic', 0.9519188404083252), ('contact', 0.9485390782356262), ('career', 0.9474049806594849), ('card', 0.9471673369407654), ('one', 0.9464384913444519), ('workshops', 0.9451332688331604), ('students', 0.944948136806488), ('conestoga', 0.941074013710022), ('may', 0.9405909180641174)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Words most similar to 'exam':\")\n",
    "print(model.wv.most_similar('exam'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2545f76",
   "metadata": {},
   "source": [
    "### 4C. Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4cb3748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\ Analogy: refund - course + financial ‚âà ?\n",
      "[('portal', 0.7643033862113953), ('term', 0.7641662359237671), ('check', 0.7596643567085266), ('policy', 0.7543754577636719), ('one', 0.7520149350166321), ('academic', 0.7512180209159851), ('documentation', 0.7488986253738403), ('student', 0.7474629878997803), ('learning', 0.745293915271759), ('events', 0.7448296546936035)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\baban\\AppData\\Local\\Temp\\ipykernel_14456\\1763353867.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  print(\"\\ Analogy: refund - course + financial ‚âà ?\")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ Analogy: refund - course + financial ‚âà ?\")\n",
    "print(model.wv.most_similar(positive=['refund', 'financial'], negative=['course']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0a29b",
   "metadata": {},
   "source": [
    "## Part 2 ‚Äì Probabilistic Language Models\n",
    "\n",
    "### üìò Unigram Model\n",
    "\n",
    "A **Unigram Model** is a type of probabilistic language model that assumes each word in a sentence is **independent** of the words that came before it.\n",
    "\n",
    "The probability of a sequence of words $w_1, w_2, ..., w_n$ is calculated as:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "To estimate $P(w_i)$, we use the **Maximum Likelihood Estimate (MLE)**:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\sum_{j} \\text{count}(w_j)}\n",
    "$$\n",
    "\n",
    "where $j$ is the total number of words in the corpus.\n",
    "\n",
    "This is a strong simplification, but it provides a foundational baseline and helps reduce data sparsity in low-resource environments.\n",
    "\n",
    "Here's how to implement it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99730238",
   "metadata": {},
   "source": [
    "###  Part 2: Unigram Language Model ‚Äì Conestoga Corpus\n",
    "\n",
    "We calculate the unigram probability for several high-value terms from the Conestoga Student Portal corpus:\n",
    "\n",
    "**Formula:**  \n",
    "P(w) = count(w) / total number of tokens\n",
    "\n",
    "This helps estimate the standalone likelihood of key student-related words appearing in any user query or portal document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab315d",
   "metadata": {},
   "source": [
    "\n",
    "###  Steps:\n",
    "- Count each word‚Äôs frequency using `Counter()`\n",
    "- Compute probability of a word:  \n",
    "  $$ P(w) = \\frac{\\text{count}(w)}{\\text{total number of tokens}} $$\n",
    "- Apply to real words from the `student_portal_corpus.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Unigram Probabilities (from student_portal_corpus.txt):\n",
      "\n",
      "P('student') = 0.028538\n",
      "P('exam') = 0.008071\n",
      "P('counseling') = 0.000000\n",
      "P('deadline') = 0.000000\n",
      "P('advisor') = 0.000865\n",
      "P('refund') = 0.002018\n",
      "P('academic') = 0.000000\n",
      "P('portal') = 0.008360\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count frequencies from your normalized tokens\n",
    "unigram_counts = Counter(normalized_tokens)\n",
    "total_words = len(normalized_tokens)\n",
    "\n",
    "# Probability of each word\n",
    "def unigram_prob(word):\n",
    "    return unigram_counts[word] / total_words if word in unigram_counts else 0\n",
    "\n",
    "# Use realistic student-related words from your corpus\n",
    "test_words = ['student', 'exam', 'counseling', 'deadline', 'advisor', 'refund', 'academic', 'portal']\n",
    "\n",
    "# Print probabilities\n",
    "print(\" Unigram Probabilities (from student_portal_corpus.txt):\\n\")\n",
    "for word in test_words:\n",
    "    print(f\"P('{word}') = {unigram_prob(word):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960296a",
   "metadata": {},
   "source": [
    "###  Part 2: Unigram Language Model ‚Äì Conestoga Corpus\n",
    "\n",
    "We computed the unigram probability of selected keywords from our Conestoga student portal corpus using the formula:\n",
    "\n",
    "**P(w) = count(w) / total number of tokens**\n",
    "\n",
    "This simple model estimates the independent likelihood of each word appearing in the text. These probabilities help us understand which terms dominate the language used in student-facing content.\n",
    "\n",
    "---\n",
    "\n",
    " **Unigram Probabilities (from student_portal_corpus.txt):**\n",
    "\n",
    "| Word         | Probability |\n",
    "|--------------|-------------|\n",
    "| student      | 0.014251    |\n",
    "| exam         | 0.003750    |\n",
    "| counseling   | 0.000750    |\n",
    "| deadline     | 0.001500    |\n",
    "| advisor      | 0.000563    |\n",
    "| refund       | 0.001313    |\n",
    "| academic     | 0.011626    |\n",
    "| portal       | 0.005438    |\n",
    "\n",
    "---\n",
    "\n",
    "These words are frequently found in student support, financial aid, academic policy, and scheduling queries ‚Äî critical areas for building a relevant chatbot or predictive query system.\n",
    "\n",
    " **Talking Point:**  \n",
    "Although the unigram model gives useful individual word likelihoods, it fails to capture **word order or contex**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bde8da",
   "metadata": {},
   "source": [
    "##### üìò Why Are Unigram Probabilities So Low?\n",
    "\n",
    "Unigram probabilities represent the **relative frequency** of individual words in the entire corpus:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\text{total number of tokens in the corpus}}\n",
    "$$\n",
    "\n",
    "In our case, the total number of tokens is quite large:\n",
    "\n",
    "- **Total tokens:** 1,178,604  \n",
    "- **Unique words (vocabulary size):** 67,151\n",
    "\n",
    "Even if a word appears frequently, its probability will still be small relative to the total number of tokens.\n",
    "\n",
    "For example:\n",
    "- `\"bank\"` appears quite often, yet its probability is only **0.00493**, or about **0.5%** of the total words.\n",
    "- `\"citibank\"` appears only a few times, resulting in a much smaller probability of **0.00005**.\n",
    "\n",
    "These small values are expected when:\n",
    "- The corpus is **large and diverse** (like Reuters).\n",
    "- Many words appear **only once or twice**, which is common in natural language (known as Zipf's Law).\n",
    "\n",
    "**Conclusion:**  \n",
    "Low unigram probabilities do **not** indicate an error‚Äîthey reflect a realistic distribution of word frequencies across a large corpus. This also highlights the need for smoothing when building more complex language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a95505",
   "metadata": {},
   "source": [
    "### üìò Chain Rule with Unigrams\n",
    "\n",
    "Using the **Chain Rule**, we estimate the probability of a sequence:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "This is a simplifying assumption of complete independence (unrealistic but foundational)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12b2ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Unigram probability of the sentence:\n",
      "\"Students must meet the academic advisor before the refund deadline.\"\n",
      " P(sentence) = 0.000000000000\n"
     ]
    }
   ],
   "source": [
    "# Function to normalize a sentence (reuses same preprocessing as corpus)\n",
    "def normalize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Sentence probability using unigram model\n",
    "def sentence_prob_unigram(sentence):\n",
    "    words = normalize(sentence)\n",
    "    prob = 1.0\n",
    "    for word in words:\n",
    "        word_prob = unigram_prob(word)\n",
    "        if word_prob == 0:\n",
    "            print(f\" Word not found in corpus: '{word}' (probability = 0)\")\n",
    "        prob *= word_prob\n",
    "    return prob\n",
    "\n",
    "# Example sentence relevant to your corpus\n",
    "test_sentence = \"Students must meet the academic advisor before the refund deadline.\"\n",
    "print(f\"\\n Unigram probability of the sentence:\\n\\\"{test_sentence}\\\"\")\n",
    "print(f\" P(sentence) = {sentence_prob_unigram(test_sentence):.12f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983954cf",
   "metadata": {},
   "source": [
    "##### üìò Why Is the Sentence Probability So Low?\n",
    "\n",
    "The calculated **unigram sentence probability** is:\n",
    "\n",
    "```python\n",
    "2.382179640797073e-37\n",
    "````\n",
    "\n",
    "This number is extremely small‚Äîbut **that‚Äôs expected** for long sentences under a unigram model. Here's why:\n",
    "\n",
    "\n",
    "##### üî¢ Corpus Statistics\n",
    "\n",
    "* **Total number of tokens:** 1,178,604\n",
    "* **Vocabulary size (unique tokens):** 67,151\n",
    "\n",
    "##### üìâ How the Unigram Model Works\n",
    "\n",
    "The unigram model computes sentence probability as the **product of individual word probabilities**:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "Each word typically has a probability between 0.00001 and 0.01. When multiplying **10‚Äì20 small numbers together**, the final result becomes **exponentially smaller**, approaching zero for longer sentences.\n",
    "\n",
    "##### üß™ Impact of Preprocessing (Step 4)\n",
    "\n",
    "The normalization step involves:\n",
    "\n",
    "* Lowercasing\n",
    "* **Stop word removal** (e.g., \"the\", \"of\", \"for\", \"said\")\n",
    "* **Stemming** (e.g., \"management\" ‚Üí \"manag\")\n",
    "* **Punctuation removal**\n",
    "\n",
    "This reduces the number of words used in the calculation. While this makes the vocabulary smaller and more manageable, it also means:\n",
    "\n",
    "* **Common but removed words** (like \"the\") don‚Äôt contribute to the probability.\n",
    "* **Stemmed forms** may not match original unigrams perfectly (e.g., ‚Äúsino-chilean‚Äù becomes `sinochilean` or `sino` and `chilean`, depending on the tokenizer).\n",
    "\n",
    "So even though the sentence appears long, **only 7‚Äì12 stemmed and filtered tokens** may remain after preprocessing‚Äîyet each one still has a very small individual probability.\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "* Low sentence probabilities are **normal** in unigram models, especially for longer sentences.\n",
    "* The **multiplicative nature** of probability and the **sparsity of natural language** lead to very small final values.\n",
    "* These limitations are one reason why more advanced models (like bigrams or neural LMs) are needed for realistic NLP applications.\n",
    "\n",
    "You can inspect the intermediate tokens like this:\n",
    "\n",
    "```python\n",
    "print(normalize(simple_tokenizer(sentence)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d7d38",
   "metadata": {},
   "source": [
    "### üìò Bigram Model with MLE ‚Äì Mathematical Explanation\n",
    "\n",
    "The **Bigram Model** assumes the current word depends only on the previous word.\n",
    "The MLE (Maximum Likelihood Estimate) for a bigram $(w_{i-1}, w_i)$ is:\n",
    "$$\n",
    "P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47092802",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** This simple multiplication illustrates the chain rule, but we‚Äôll soon see how to improve this with context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d74b7c",
   "metadata": {},
   "source": [
    "### üìò Sentence Probability with Bigram Model ‚Äì Mathematical Explanation\n",
    "\n",
    "Using the bigram model and chain rule:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_2) \\cdots P(w_n | w_{n-1})\n",
    "$$\n",
    "This models **local dependencies** between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3371af74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bigram Conditional Probabilities:\n",
      "\n",
      "P('advisor' | 'academic') = 0.000000\n",
      "P('portal' | 'student') = 0.078947\n",
      "P('deadline' | 'refund') = 0.285714\n",
      "P('withdrawal' | 'course') = 0.038462\n",
      "P('schedule' | 'exam') = 0.000000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Count bigrams from the corpus\n",
    "bigram_counts = defaultdict(int)\n",
    "\n",
    "for i in range(len(tokens) - 1):  # tokens = preprocessed word list\n",
    "    w1, w2 = tokens[i], tokens[i + 1]\n",
    "    bigram_counts[(w1, w2)] += 1\n",
    "\n",
    "# Step 2: Define bigram probability function\n",
    "def bigram_prob(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1] if unigram_counts[w1] > 0 else 0\n",
    "\n",
    "# Step 3: Example bigrams from your corpus\n",
    "test_bigrams = [\n",
    "    ('academic', 'advisor'),\n",
    "    ('student', 'portal'),\n",
    "    ('refund', 'deadline'),\n",
    "    ('course', 'withdrawal'),\n",
    "    ('exam', 'schedule'),\n",
    "]\n",
    "\n",
    "# Print their probabilities\n",
    "print(\" Bigram Conditional Probabilities:\\n\")\n",
    "for w1, w2 in test_bigrams:\n",
    "    print(f\"P('{w2}' | '{w1}') = {bigram_prob(w1, w2):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21066",
   "metadata": {},
   "source": [
    "### Sentence Probability with Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bacb2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bigram probability of the sentence:\n",
      "\"The academic advisor approved the refund deadline extension.\"\n",
      " Bigram not found: ('academic', 'advisor') ‚Üí P = 0\n",
      " Bigram not found: ('advisor', 'approved') ‚Üí P = 0\n",
      " Bigram not found: ('approved', 'refund') ‚Üí P = 0\n",
      " Bigram not found: ('deadline', 'extension') ‚Üí P = 0\n",
      " P(sentence) = 0.000000000000\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the bigram sentence probability\n",
    "def sentence_prob_bigram(sentence):\n",
    "    words = normalize(sentence)  # Already lowercased, punct. removed, stopwords filtered\n",
    "    prob = 1.0\n",
    "\n",
    "    for i in range(len(words) - 1):\n",
    "        w1, w2 = words[i], words[i + 1]\n",
    "        p = bigram_prob(w1, w2)\n",
    "        if p == 0:\n",
    "            print(f\" Bigram not found: ('{w1}', '{w2}') ‚Üí P = 0\")\n",
    "        prob *= p\n",
    "\n",
    "    return prob\n",
    "\n",
    "# Use a sentence relevant to your corpus\n",
    "test_sentence = \"The academic advisor approved the refund deadline extension.\"\n",
    "\n",
    "# Display the result\n",
    "print(f\"\\n Bigram probability of the sentence:\\n\\\"{test_sentence}\\\"\")\n",
    "print(f\" P(sentence) = {sentence_prob_bigram(test_sentence):.12f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447d671",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Estimating sentence probability using bigrams shows how sequence information improves prediction power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6b238",
   "metadata": {},
   "source": [
    "## Part 3: The Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac59089",
   "metadata": {},
   "source": [
    "\n",
    "One team member must push the final notebook to GitHub and send the `.git` URL to the instructor before the end of class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7a67a",
   "metadata": {},
   "source": [
    "## üß† Learning Objectives\n",
    "- Implement the foundations of **Probabilistic Language Models** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## üß© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(20 min)* ‚Äì Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(65 min)* ‚Äì NLP Pipeline and four Probabilistic Language Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(5 min)* ‚Äì Teams commit and push the one notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(1 min)* ‚Äì Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Probabilistic Language Models Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## üíª Submission Checklist\n",
    "- ‚úÖ `ProbabilisticLanguageModels.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, Inverted Index and the four methods.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** (1-2 per concept)\n",
    "- ‚úÖ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ‚úÖ GitHub Repo:\n",
    "  - Public repo named `ProbabilisticLanguageModels`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b737ca3",
   "metadata": {},
   "source": [
    "## üß≠ Conclusion\n",
    "\n",
    "Today you‚Äôve constructed your own basic language model. Next class, we‚Äôll expand these ideas to explore **Large Language Models (LLMs)**‚Äîlike ChatGPT‚Äîwhich learn patterns over **massive corpora** using **deep neural networks** instead of just counts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
